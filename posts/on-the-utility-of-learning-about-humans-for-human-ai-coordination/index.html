<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>On the Utility of Learning About Humans for Human Ai Coordination | Raffaelbdl.github.io</title><meta name=keywords content="human-ai-coordination,reinforcement-learning,beginner-friendly"><meta name=description content="Paper : http://arxiv.org/abs/1910.05789
The coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.
Indeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms."><meta name=author content><link rel=canonical href=https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://raffaelbdl.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://raffaelbdl.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://raffaelbdl.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://raffaelbdl.github.io/apple-touch-icon.png><link rel=mask-icon href=https://raffaelbdl.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="On the Utility of Learning About Humans for Human Ai Coordination"><meta property="og:description" content="Paper : http://arxiv.org/abs/1910.05789
The coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.
Indeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms."><meta property="og:type" content="article"><meta property="og:url" content="https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-06T19:40:16+09:00"><meta property="article:modified_time" content="2022-10-06T19:40:16+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="On the Utility of Learning About Humans for Human Ai Coordination"><meta name=twitter:description content="Paper : http://arxiv.org/abs/1910.05789
The coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.
Indeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://raffaelbdl.github.io/posts/"},{"@type":"ListItem","position":2,"name":"On the Utility of Learning About Humans for Human Ai Coordination","item":"https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"On the Utility of Learning About Humans for Human Ai Coordination","name":"On the Utility of Learning About Humans for Human Ai Coordination","description":"Paper : http://arxiv.org/abs/1910.05789\nThe coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.\nIndeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms.","keywords":["human-ai-coordination","reinforcement-learning","beginner-friendly"],"articleBody":"Paper : http://arxiv.org/abs/1910.05789\nThe coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.\nIndeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms.\nFigure 1 from the paper, it shows how human underperformance affects the agent both in adversarial and in cooperative gamee To address this issue, the authors have decided to introduce two Behavior Cloning (BC) models : one for training and the other one for testing. The goal is to encapsulate a fraction of the human behavior in a policy.\nIn parallel, they have chosen several reinforcement learning algorithms, in particular those known for their robustness. That is because they suspect that agents trained only by self-play are not confronted to enough game states and therefore do not adapt to a noisy human behavior. The chosen algorithms are PPO (trained in self-play or with the BC agent), PBT (Population Based Training), and classic planification algorithms.\nThe used environment is a simplification of the Overcooked game. The action space is restrained in order to force cooperation.\nFigure 3 from the paper, it shows different layouts of the game, especially some are circular and require a good synchronisation between the two playerse The results show that agents trained without a glimpse at human behavior (self-play agents for instance) systematically underperform.\nFigure 4 from the paper, the crossed bars correspond to trainings where the original position of the agents are reversed. This is significative in asymmetrical layouts. There are a few more points addressed in this paper.\nFirst the agents trained with a BC partner can adapt faster to human actions. They can realised more tasks in a given layout compared to self-play agents that appear to be hyper-specialized. Moreover, agents trained with a BC partner do not tend to impose a behavior to the agent, as the rotation direction in a circular layout. This make sense since the agent may have to learn how to compensate for the human failures, and therefore can not expect the human to perform however it wants they to.\nThey also observe that finetuning self-play agents by progressively putting them with more BC partners can help obtain higher results.\nThey argue that humans possess an ability to adapt to the agent behavior, which the former does not take into account. The authors suggests that training directly on the current human behavior could help.\nUltimately, this kind of study raises the issue of a standardized benchmark. In this paper, the authors asked 40 real human to play with agents. However, it makes the results are not reproductible. Indeed, it is almost impossible to make all the same people join again to test a new algorithm, and they could also have become better in the meantime. The question of a relevant standardized benchmark is thus still open.\nFigure 6 from the paper, it shows the performance of the agents when partnered with humans. ","wordCount":"529","inLanguage":"en","datePublished":"2022-10-06T19:40:16+09:00","dateModified":"2022-10-06T19:40:16+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/"},"publisher":{"@type":"Organization","name":"Raffaelbdl.github.io","logo":{"@type":"ImageObject","url":"https://raffaelbdl.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://raffaelbdl.github.io/ accesskey=h title="Raffaelbdl.github.io (Alt + H)">Raffaelbdl.github.io</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://raffaelbdl.github.io/about-me title="about me"><span>About Me</span></a></li><li><a href=https://raffaelbdl.github.io/blog/ title=blog><span>Blog</span></a></li><li><a href=https://raffaelbdl.github.io/posts/ title=posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>On the Utility of Learning About Humans for Human Ai Coordination</h1><div class=post-meta><span title='2022-10-06 19:40:16 +0900 +0900'>October 6, 2022</span></div></header><div class=post-content><p>Paper : <a href=http://arxiv.org/abs/1910.05789>http://arxiv.org/abs/1910.05789</a></p><p>The coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.</p><p>Indeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=/resources/paper-explained/overcooked1.png alt=img1></th></tr></thead><tbody><tr><td style=text-align:center>Figure 1 from the paper, it shows how human underperformance affects the agent both in adversarial and in cooperative gamee</td></tr></tbody></table><p>To address this issue, the authors have decided to introduce two Behavior Cloning (BC) models : one for training and the other one for testing. The goal is to encapsulate a fraction of the human behavior in a policy.</p><p>In parallel, they have chosen several reinforcement learning algorithms, in particular those known for their robustness. That is because they suspect that agents trained only by self-play are not confronted to enough game states and therefore do not adapt to a noisy human behavior. The chosen algorithms are PPO (trained in self-play or with the BC agent), PBT (Population Based Training), and classic planification algorithms.</p><p>The used environment is a simplification of the Overcooked game. The action space is restrained in order to force cooperation.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=/resources/paper-explained/overcooked2.png alt=img2></th></tr></thead><tbody><tr><td style=text-align:center>Figure 3 from the paper, it shows different layouts of the game, especially some are circular and require a good synchronisation between the two playerse</td></tr></tbody></table><p>The results show that agents trained without a glimpse at human behavior (self-play agents for instance) systematically underperform.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=/resources/paper-explained/overcooked3.png alt=img3></th></tr></thead><tbody><tr><td style=text-align:center>Figure 4 from the paper, the crossed bars correspond to trainings where the original position of the agents are reversed. This is significative in asymmetrical layouts.</td></tr></tbody></table><p>There are a few more points addressed in this paper.</p><p>First the agents trained with a BC partner can adapt faster to human actions. They can realised more tasks in a given layout compared to self-play agents that appear to be hyper-specialized. Moreover, agents trained with a BC partner do not tend to impose a behavior to the agent, as the rotation direction in a circular layout.
This make sense since the agent may have to learn how to compensate for the human failures, and therefore can not expect the human to perform however it wants they to.</p><p>They also observe that finetuning self-play agents by progressively putting them with more BC partners can help obtain higher results.</p><p>They argue that humans possess an ability to adapt to the agent behavior, which the former does not take into account. The authors suggests that training directly on the current human behavior could help.</p><p>Ultimately, this kind of study raises the issue of a standardized benchmark. In this paper, the authors asked 40 real human to play with agents. However, it makes the results are not reproductible. Indeed, it is almost impossible to make all the same people join again to test a new algorithm, and they could also have become better in the meantime. The question of a relevant standardized benchmark is thus still open.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=/resources/paper-explained/overcooked4.png alt=img4></th></tr></thead><tbody><tr><td style=text-align:center>Figure 6 from the paper, it shows the performance of the agents when partnered with humans.</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://raffaelbdl.github.io/tags/human-ai-coordination/>human-ai-coordination</a></li><li><a href=https://raffaelbdl.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://raffaelbdl.github.io/tags/beginner-friendly/>beginner-friendly</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://raffaelbdl.github.io/>Raffaelbdl.github.io</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>