<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>human-ai-coordination on Raffaelbdl.github.io</title><link>https://raffaelbdl.github.io/tags/human-ai-coordination/</link><description>Recent content in human-ai-coordination on Raffaelbdl.github.io</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 06 Oct 2022 19:40:16 +0900</lastBuildDate><atom:link href="https://raffaelbdl.github.io/tags/human-ai-coordination/index.xml" rel="self" type="application/rss+xml"/><item><title>On the Utility of Learning About Humans for Human Ai Coordination</title><link>https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/</link><pubDate>Thu, 06 Oct 2022 19:40:16 +0900</pubDate><guid>https://raffaelbdl.github.io/posts/on-the-utility-of-learning-about-humans-for-human-ai-coordination/</guid><description>Paper : http://arxiv.org/abs/1910.05789
The coordination / cooperation between humans and AI can be useful in many applications like robotics or video games. But this task proves to be difficult.
Indeed, when a reinforcement learning agent meets a human it is usually in an adversarial context (chess for instance). Therefore if the human performs poorly, it makes it easier for the agent. Conversely in a cooperation framework, the agent must take into account that the human underperforms.</description></item></channel></rss>